{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Get variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "keyvaultlsname = 'Ls_KeyVault_01'\n",
        "adls2lsname = 'Ls_AdlsGen2_01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Linked Services Setup: KV and ADLS Gen2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_39984\\3989627984.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtoken_library\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmicrosoft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynapse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenlibrary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokenLibrary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstorage_account\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_library\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSecretWithLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyvaultlsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"datalakeaccountname\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "storage_account = token_library.getSecretWithLS(keyvaultlsname, \"datalakeaccountname\")\n",
        "\n",
        "spark.conf.set(\"spark.storage.synapse.linkedServiceName\", adls2lsname)\n",
        "spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3. Create Schemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-13T15:02:36.8076517Z",
              "execution_start_time": "2021-10-13T15:02:35.7403753Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T15:02:35.5548536Z",
              "session_id": 40,
              "session_start_time": null,
              "spark_pool": "synspdevdep70",
              "state": "finished",
              "statement_id": 9
            },
            "text/plain": [
              "StatementMeta(synspdevdep70, 40, 9, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS dw LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS lnd LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS interim LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS malformed LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 4. Create Fact Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-13T14:39:01.171823Z",
              "execution_start_time": "2021-10-13T14:38:58.3874423Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T14:38:58.2071314Z",
              "session_id": 40,
              "session_start_time": null,
              "spark_pool": "synspdevdep70",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(synspdevdep70, 40, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "spark.sql(f\"DROP TABLE IF EXISTS dw.fact_parking\")\n",
        "\n",
        "spark.sql(f\"CREATE TABLE dw.fact_parking(dim_date_id STRING,dim_time_id STRING, dim_parking_bay_id STRING, dim_location_id STRING, dim_st_marker_id STRING, status STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/fact_parking/'\")\n",
        " \n",
        "spark.sql(f\"REFRESH TABLE dw.fact_parking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 5. Create Dimension Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-13T14:39:12.438811Z",
              "execution_start_time": "2021-10-13T14:39:03.8580484Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T14:39:03.7430402Z",
              "session_id": 40,
              "session_start_time": null,
              "spark_pool": "synspdevdep70",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(synspdevdep70, 40, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "spark.sql(f\"DROP TABLE IF EXISTS dw.dim_st_marker\")\n",
        "spark.sql(f\"CREATE TABLE dw.dim_st_marker(dim_st_marker_id STRING, st_marker_id STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_st_marker/'\")\n",
        "spark.sql(f\"REFRESH TABLE dw.dim_st_marker\")\n",
        " \n",
        "\n",
        "spark.sql(f\"DROP TABLE IF EXISTS dw.dim_location\")\n",
        "spark.sql(f\"CREATE TABLE dw.dim_location(dim_location_id STRING,lat FLOAT, lon FLOAT, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_location/'\")\n",
        "spark.sql(f\"REFRESH TABLE dw.dim_location\")\n",
        " \n",
        "\n",
        "spark.sql(f\"DROP TABLE IF EXISTS dw.dim_parking_bay\")\n",
        "spark.sql(f\"CREATE TABLE dw.dim_parking_bay(dim_parking_bay_id STRING, bay_id INT,`marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/dw/dim_parking_bay/'\")\n",
        "spark.sql(f\"REFRESH TABLE dw.dim_parking_bay\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 6. Create dim date and time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-13T14:39:49.1987436Z",
              "execution_start_time": "2021-10-13T14:39:15.5201846Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T14:39:15.4090129Z",
              "session_id": 40,
              "session_start_time": null,
              "spark_pool": "synspdevdep70",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(synspdevdep70, 40, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "spark.sql(f\"DROP TABLE IF EXISTS dw.dim_date\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS dw.dim_time\")\n",
        "\n",
        "# DimDate\n",
        "dimdate = spark.read.csv(f\"abfss://datalake@{storage_account}.dfs.core.windows.net/data/seed/dim_date/dim_date.csv\", header=True)\n",
        "dimdate.write.saveAsTable(\"dw.dim_date\")\n",
        "\n",
        "# DimTime\n",
        "dimtime = spark.read.csv(f\"abfss://datalake@{storage_account}.dfs.core.windows.net/data/seed/dim_time/dim_time.csv\", header=True)\n",
        "dimtime.write.saveAsTable(\"dw.dim_time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 7. Create interim and error tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2021-10-13T14:40:26.1451789Z",
              "execution_start_time": "2021-10-13T14:40:15.5722577Z",
              "livy_statement_state": "available",
              "queued_time": "2021-10-13T14:40:15.4586215Z",
              "session_id": 40,
              "session_start_time": null,
              "spark_pool": "synspdevdep70",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(synspdevdep70, 40, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "spark.sql(f\"DROP TABLE IF EXISTS interim.parking_bay\")\n",
        "spark.sql(f\"CREATE TABLE interim.parking_bay(bay_id INT, `last_edit` TIMESTAMP, `marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, `the_geom` STRUCT<`coordinates`: ARRAY<ARRAY<ARRAY<ARRAY<DOUBLE>>>>, `type`: STRING>, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/interim/interim.parking_bay/'\")\n",
        "spark.sql(f\"REFRESH TABLE interim.parking_bay\")\n",
        "\n",
        "spark.sql(f\"DROP TABLE IF EXISTS interim.sensor\")\n",
        "spark.sql(f\"CREATE TABLE  interim.sensor(bay_id INT, `st_marker_id` STRING, `lat` FLOAT, `lon` FLOAT, `location` STRUCT<`coordinates`: ARRAY<DOUBLE>, `type`: STRING>, `status` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/interim/interim.sensor/'\")\n",
        "spark.sql(f\"REFRESH TABLE  interim.sensor\")\n",
        "   \n",
        "\n",
        "spark.sql(f\"DROP TABLE IF EXISTS malformed.parking_bay\")\n",
        "spark.sql(f\"CREATE TABLE malformed.parking_bay(bay_id INT, `last_edit` TIMESTAMP,`marker_id` STRING, `meter_id` STRING, `rd_seg_dsc` STRING, `rd_seg_id` STRING, `the_geom` STRUCT<`coordinates`: ARRAY<ARRAY<ARRAY<ARRAY<DOUBLE>>>>, `type`: STRING>, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/malformed/malformed.parking_bay/'\")\n",
        "spark.sql(f\"REFRESH TABLE malformed.parking_bay\")\n",
        "\n",
        "spark.sql(f\"DROP TABLE IF EXISTS malformed.sensor\")\n",
        "spark.sql(f\"CREATE TABLE malformed.sensor(bay_id INT,`st_marker_id` STRING,`lat` FLOAT,`lon` FLOAT,`location` STRUCT<`coordinates`: ARRAY<DOUBLE>, `type`: STRING>,`status` STRING, load_id STRING, loaded_on TIMESTAMP) USING parquet LOCATION 'abfss://datalake@{storage_account}.dfs.core.windows.net/data/malformed/malformed.parking_bay/'\")\n",
        "spark.sql(f\"REFRESH TABLE malformed.sensor\")"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
