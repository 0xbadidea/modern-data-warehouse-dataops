{
	"name": "03_silver_update",
	"properties": {
		"folder": {
			"name": "Delta/pipelines"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "9a0a0fa6-51a8-432d-a730-364f918fbf70"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%md-sandbox\n",
					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Silver Table Updates\n",
					"\n",
					"We have processed data from the Bronze table to the Silver table.\n",
					"We need to do some updates to ensure high quality in the Silver\n",
					"table.\n",
					"\n",
					"😎 We're reading _from_ the Delta table now because a Delta table can be both a source AND a sink."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Notebook Objective\n",
					"\n",
					"In this notebook we:\n",
					"1. Harden the Raw to Bronze and Bronze to Silver Steps we wrote in a\n",
					"   previous notebook.\n",
					"1. Diagnose data quality issues.\n",
					"1. Update the broken readings in the Silver table.\n",
					"1. Handle late-arriving data."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Step Configuration"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ./includes/configuration"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Import Operation Functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ./includes/main/python/operations"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Display the Files in the Raw and Bronze Paths"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"display(dbutils.fs.ls(rawPath))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"display(dbutils.fs.ls(bronzePath))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Start Streams\n",
					"\n",
					"Before we add new streams, let's start the streams we have previously engineered.\n",
					"\n",
					"We will start two named streams:\n",
					"\n",
					"- `write_raw_to_bronze`\n",
					"- `write_bronze_to_silver`"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Current Delta Architecture\n",
					"Next, we demonstrate everything we have built up to this point in our\n",
					"Delta Architecture.\n",
					"\n",
					"Again, we do so with composable functions included in the\n",
					"file `includes/main/python/operations`."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### The Hardened Raw to Bronze Pipeline"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"rawDF = read_stream_raw(spark, rawPath)\n",
					"transformedRawDF = transform_raw(rawDF)\n",
					"rawToBronzeWriter = create_stream_writer(\n",
					"    dataframe=transformedRawDF,\n",
					"    checkpoint=bronzeCheckpoint,\n",
					"    name=\"write_raw_to_bronze\",\n",
					"    partition_column=\"p_ingestdate\",\n",
					")\n",
					"rawToBronzeWriter.start(bronzePath)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### The Hardened Bronze to Silver Pipeline"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"bronzeDF = read_stream_delta(spark, bronzePath)\n",
					"transformedBronzeDF = transform_bronze(bronzeDF)\n",
					"bronzeToSilverWriter = create_stream_writer(\n",
					"    dataframe=transformedBronzeDF,\n",
					"    checkpoint=silverCheckpoint,\n",
					"    name=\"write_bronze_to_silver\",\n",
					"    partition_column=\"p_eventdate\",\n",
					")\n",
					"bronzeToSilverWriter.start(silverPath)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Show Running Streams"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"for stream in spark.streams.active:\n",
					"    print(stream.name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Diagnose Data Quality Issues\n",
					"\n",
					"It is a good idea to perform quality checking on the data - such as looking for and reconciling anomalies - as well as further transformations such as cleaning and/or enriching the data.\n",
					"\n",
					"In a visualization in the previous notebook, we noticed:\n",
					"\n",
					"1. the table is missing records.\n",
					"1. the presence of some negative recordings even though negative heart rates are impossible.\n",
					"\n",
					"Let's assess the extent of the negative reading anomalies."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Create a Temporary View of the Broken Readings in the Silver Table\n",
					"\n",
					"Display a count of the number of records for each day in the Silver\n",
					"table where the measured heartrate is negative."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"\n",
					"from pyspark.sql.functions import count\n",
					"\n",
					"broken_readings = (\n",
					"    spark.read.format(\"delta\")\n",
					"    .load(silverPath)\n",
					"    .select(col(\"heartrate\"), col(\"p_eventdate\"))\n",
					"    .where(col(\"heartrate\") < 0)\n",
					"    .groupby(\"p_eventdate\")\n",
					"    .agg(count(\"heartrate\"))\n",
					"    .orderBy(\"p_eventdate\")\n",
					")\n",
					"\n",
					"broken_readings.createOrReplaceTempView(\"broken_readings\")\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT * FROM broken_readings"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT SUM(`count(heartrate)`) FROM broken_readings"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"We have identified two issues with the Silver table:\n",
					"\n",
					"1. There are missing records\n",
					"1. There are records with broken readings\n",
					"\n",
					"Let's update the table."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Update the Broken Readings\n",
					"To update the broken sensor readings (heartrate less than zero), we'll interpolate using the value recorded before and after for each device. The `pyspark.sql` functions `lag()` and `lead()` will make this a trivial calculation.\n",
					"In order to use these functions, we need to import the pyspark.sql.window function `Window`. This will allow us to create a date window consisting of the dates immediately before and after our missing value.\n",
					"\n",
					"🚎Window functions operate on a group of rows, referred to as a window, and calculate a return value for each row based on the group of rows. Window functions are useful for processing tasks such as calculating a moving average, computing a cumulative statistic, or accessing the value of rows given the relative position of the current row.\n",
					"\n",
					"We'll write these values to a temporary view called `updates`. This view will be used later to upsert values into our Silver Delta table.\n",
					"\n",
					"[pyspark.sql window functions documentation](https://spark.apache.org/docs/3.0.0/sql-ref-functions-builtin.html#window-functions)"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create a DataFrame that Interpolates the Broken Values"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.window import Window\n",
					"from pyspark.sql.functions import col, lag, lead\n",
					"\n",
					"dateWindow = Window.orderBy(\"p_eventdate\")\n",
					"\n",
					"interpolatedDF = spark.read.table(\"health_tracker_plus_silver\").select(\n",
					"    \"*\",\n",
					"    lag(col(\"heartrate\")).over(dateWindow).alias(\"prev_amt\"),\n",
					"    lead(col(\"heartrate\")).over(dateWindow).alias(\"next_amt\"),\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Create a DataFrame of Updates"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"updatesDF = interpolatedDF.where(col(\"heartrate\") < 0).select(\n",
					"    \"device_id\",\n",
					"    ((col(\"prev_amt\") + col(\"next_amt\")) / 2).alias(\"heartrate\"),\n",
					"    \"eventtime\",\n",
					"    \"name\",\n",
					"    \"p_eventdate\",\n",
					")\n",
					"\n",
					"display(updatesDF)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Write an assertion to verify that the Silver table and the UpdatesDF have the same schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"assert (\n",
					"    spark.read.table(\"health_tracker_plus_silver\").schema == updatesDF.schema\n",
					"), \"Schemas do not match\"\n",
					"print(\"Assertion passed.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Update the Silver Table"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from delta.tables import DeltaTable\n",
					"\n",
					"silverTable = DeltaTable.forPath(spark, silverPath)\n",
					"\n",
					"update_match = \"\"\"\n",
					"  health_tracker.eventtime = updates.eventtime\n",
					"  AND\n",
					"  health_tracker.device_id = updates.device_id\n",
					"\"\"\"\n",
					"\n",
					"update = {\"heartrate\": \"updates.heartrate\"}\n",
					"\n",
					"(\n",
					"    silverTable.alias(\"health_tracker\")\n",
					"    .merge(updatesDF.alias(\"updates\"), update_match)\n",
					"    .whenMatchedUpdate(set=update)\n",
					"    .execute()\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Handle Late-Arriving Data\n",
					"\n",
					"🤦🏼‍It turns out that our expectation of receiving the missing records late was correct. The complete month of February has subsequently been made available to us."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"retrieve_data(2020, 2, rawPath, is_late=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"display(dbutils.fs.ls(rawPath + \"/late\"))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Count the records in the late file\n",
					"\n",
					"The late file is a json file in the `rawPath + \"late\"` directory."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"spark.read.json(rawPath + \"/late\").count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"🧐 You should note that the late file has all the records from the month of February, a count of 3480.\n",
					"\n",
					"❗️ If we simply append this file to the Bronze Delta table, it will create many duplicate entries."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Read the Late File\n",
					"\n",
					"Next we read in the late file. Note that we make use of the `transform_raw` function loaded from the `includes/main/python/operations` notebook."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"kafka_schema = \"value STRING\"\n",
					"\n",
					"lateRawDF = spark.read.format(\"text\").schema(kafka_schema).load(rawPath + \"/late\")\n",
					"\n",
					"transformedLateRawDF = transform_raw(lateRawDF)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Merge the Late-Arriving Data with the Bronze Table\n",
					"\n",
					"We use the special method `.whenNotMatchedInsertAll` to insert only the records that are not present in the Bronze table. This is a best practice for preventing duplicate entries in a Delta table."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"bronzeTable = DeltaTable.forPath(spark, bronzePath)\n",
					"\n",
					"existing_record_match = \"bronze.value = latearrivals.value\"\n",
					"\n",
					"(\n",
					"    bronzeTable.alias(\"bronze\")\n",
					"    .merge(transformedLateRawDF.alias(\"latearrivals\"), existing_record_match)\n",
					"    .whenNotMatchedInsertAll()\n",
					"    .execute()\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Write An Aggregation on the Silver table\n",
					"\n",
					"### Count the number of records in the Silver table for each device id\n",
					"\n",
					"💪🏼 The Silver table is registered in the Metastore as `health_tracker_plus_silver`.\n",
					"\n",
					"👀 **Hint**: We did this exact query in the previous notebook."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"\n",
					"untilStreamIsReady(\"write_bronze_to_silver\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"from pyspark.sql.functions import count\n",
					"\n",
					"display(\n",
					"    spark.read.table(\"health_tracker_plus_silver\").groupby(\"device_id\").agg(count(\"*\"))\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Troubleshooting\n",
					"\n",
					"😫 If you run this query before the stream from the Bronze to the Silver tables has been picked up you will still see missing records for `device_id`: 4.\n",
					"\n",
					"Wait a moment and run the query again."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Check Yourself\n",
					"\n",
					"You should see that there are an equal number of entries, 1440, for each device id."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Table Histories"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"display(bronzeTable.history())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"display(silverTable.history())"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Time Travel\n",
					"We can query an earlier version of the Delta table using the time travel feature. By running the following two cells, we can see that the current table count is larger than it was before we ingested the new data file into the stream."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT COUNT(*) FROM health_tracker_plus_silver VERSION AS OF 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT COUNT(*) FROM health_tracker_plus_silver VERSION AS OF 1"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT COUNT(*) FROM health_tracker_plus_silver VERSION AS OF 2"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Stop All Streams\n",
					"\n",
					"In the next notebook, we will build the Silver to Gold Step.\n",
					"\n",
					"Before we do so, let's shut down all streams in this notebook."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"stop_all_streams()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"&copy; 2020 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}