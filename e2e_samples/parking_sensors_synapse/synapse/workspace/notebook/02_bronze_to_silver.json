{
	"name": "02_bronze_to_silver",
	"properties": {
		"folder": {
			"name": "Delta/pipelines"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d3be9766-97d9-4b91-ac3e-82d6dfb74fc2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%md-sandbox\n",
					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Bronze to Silver - ETL into a Silver table\n",
					"\n",
					"We need to perform some transformations on the data to move it from bronze to silver tables.\n",
					"\n",
					"üòé We're reading _from_ the Delta table now because a Delta table can be both a source AND a sink."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Notebook Objective\n",
					"\n",
					"In this notebook we:\n",
					"1. Harden the Raw to Bronze Step we wrote in a previous notebook\n",
					"2. Develop the Bronze to Silver Step\n",
					"   - Extract and Transform the Raw string to columns\n",
					"   - Load this Data into the Silver Table"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Step Configuration"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ./includes/configuration"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Import Operation Functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ./includes/main/python/operations"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Display the Files in the Raw and Bronze Paths"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"display(dbutils.fs.ls(rawPath))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"display(dbutils.fs.ls(bronzePath))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Start Streams\n",
					"\n",
					"Before we add new streams, let's start the streams we have previously engineered.\n",
					"\n",
					"We will start two named streams:\n",
					"\n",
					"- `write_raw_to_bronze`\n",
					"- `display_bronze`\n",
					"\n",
					"ü§† In a typical production setting, you would not interact with your streams as we are doing here. We stop and restart our streams in each new notebook for demonstration purposes. *It is easier to track everything that is happening if our streams are only running in our current notebook.*"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Current Delta Architecture\n",
					"Next, we demonstrate everything we have built up to this point in our\n",
					"Delta Architecture.\n",
					"\n",
					"#### Harden the Raw to Bronze Step\n",
					"\n",
					"We do so not with the ad hoc queries as written before, but now with\n",
					"composable functions included in the file `includes/main/python/operations`.\n",
					"This is a process known as **hardening** the step. If the data engineering\n",
					"code is written in composable functions, it can be unit tested to ensure\n",
					"stability.\n",
					"\n",
					"üõ† In our composable functions we will be making use of\n",
					"[Python Type Hints](https://docs.python.org/3/library/typing.html).\n",
					"\n",
					"#### Python Type Hints\n",
					"\n",
					"For example, the function below takes and returns a string and is annotated as follows:\n",
					"\n",
					"```\n",
					"def greeting(name: str) -> str:\n",
					"    return 'Hello ' + name\n",
					"```\n",
					"In the function `greeting`, the argument `name` is expected to be of type `str`\n",
					"and the return type `str`."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Step 1: Create the `rawDF` Streaming DataFrame\n",
					"\n",
					"In the previous notebook, we wrote:\n",
					"\n",
					"```\n",
					"rawDF = (\n",
					"  spark.readStream\n",
					"  .format(\"text\")\n",
					"  .schema(kafka_schema)\n",
					"  .load(rawPath)\n",
					")\n",
					"```\n",
					"\n",
					"Now, we use the following function in `includes/main/python/operations`\n",
					"\n",
					"```\n",
					"def read_stream_raw(spark: SparkSession, rawPath: str) -> DataFrame:\n",
					"  kafka_schema = \"value STRING\"\n",
					"  return (\n",
					"    spark.readStream\n",
					"    .format(\"text\")\n",
					"    .schema(kafka_schema)\n",
					"    .load(rawPath)\n",
					"  )\n",
					"```\n",
					"\n",
					"ü§© Note that we have injected the current Spark Session into the function as the variable `spark`."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"rawDF = read_stream_raw(spark, rawPath)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Step 2: Transform the Raw Data\n",
					"\n",
					"Next, we transform the raw data, `rawDF`. Previously, we wrote:\n",
					"\n",
					"```\n",
					"rawDF = (\n",
					"  rawDF.select(\n",
					"    lit(\"files.training.databricks.com\").alias(\"datasource\"),\n",
					"    current_timestamp().alias(\"ingesttime\"),\n",
					"    \"value\",\n",
					"    current_timestamp().cast(\"date\").alias(\"ingestdate\")\n",
					"  )\n",
					")\n",
					"```\n",
					"\n",
					"Now, we use the following function in `includes/main/python/operations`\n",
					"\n",
					"```\n",
					"def transform_raw(df: DataFrame) -> DataFrame:\n",
					"  return (\n",
					"    df.select(\n",
					"      lit(\"files.training.databricks.com\").alias(\"datasource\"),\n",
					"      current_timestamp().alias(\"ingesttime\"),\n",
					"      \"value\",\n",
					"      current_timestamp().cast(\"date\").alias(\"p_ingestdate\")\n",
					"    )\n",
					"  )\n",
					"```"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"transformedRawDF = transform_raw(rawDF)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Step 3: Write Stream to a Bronze Table\n",
					"\n",
					"Finally, we write to the Bronze Table using Structured Streaming.\n",
					"Previously, we wrote:\n",
					"\n",
					"```\n",
					"(\n",
					"  raw_health_tracker_data_df\n",
					"  .select(\"datasource\", \"ingesttime\", \"value\", col(\"ingestdate\").alias(\"p_ingestdate\"))\n",
					"  .writeStream\n",
					"  .format(\"delta\")\n",
					"  .outputMode(\"append\")\n",
					"  .option(\"checkpointLocation\", bronzeCheckpoint)\n",
					"  .partitionBy(\"p_ingestdate\")\n",
					"  .queryName(\"write_raw_to_bronze\")\n",
					"  .start(bronzePath)\n",
					")\n",
					"```\n",
					"Now, we use the following function in `includes/main/python/operations`\n",
					"\n",
					"```\n",
					"def create_stream_writer(dataframe: DataFrame, checkpoint: str,\n",
					"                         name: str, partition_column: str=None,\n",
					"                         mode: str=\"append\") -> DataStreamWriter:\n",
					"\n",
					"    stream_writer = (\n",
					"        dataframe.writeStream\n",
					"        .format(\"delta\")\n",
					"        .outputMode(mode)\n",
					"        .option(\"checkpointLocation\", checkpoint)\n",
					"        .queryName(name)\n",
					"    )\n",
					"    if partition_column is not None:\n",
					"      return stream_writer.partitionBy(partition_column)\n",
					"    return stream_writer\n",
					"```\n",
					"\n",
					"ü§Ø **Note**: This function will be used repeatedly, every time we create\n",
					"a `DataStreamWriter`.\n",
					"\n",
					"‚òùüèø This function returns a `DataStreamWriter`, not a `DataFrame`. This means\n",
					"that we will have to call `.start()` as a function method to start the stream."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"rawToBronzeWriter = create_stream_writer(\n",
					"    dataframe=transformedRawDF,\n",
					"    checkpoint=bronzeCheckpoint,\n",
					"    name=\"write_raw_to_bronze\",\n",
					"    partition_column=\"p_ingestdate\",\n",
					")\n",
					"\n",
					"rawToBronzeWriter.start(bronzePath)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Display the Bronze Table"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"bronzeDF = read_stream_delta(spark, bronzePath)\n",
					"display(bronzeDF, streamName=\"display_bronze\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Show Running Streams"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"for stream in spark.streams.active:\n",
					"    print(stream.name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Make Notebook Idempotent"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dbutils.fs.rm(silverPath, recurse=True)\n",
					"dbutils.fs.rm(silverCheckpoint, recurse=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Count Records in the Bronze Table\n",
					"\n",
					"Display how many records are in our table so we can watch it grow as the data streams in. As we ingest more files, you will be able to return to this streaming display and watch the count increase.\n",
					"\n",
					"- Use the DataFrame, `bronzeDF`, which is a reference to the Bronze Delta table\n",
					"- Write spark code to count the number of records in the Bronze Delta table\n",
					"\n",
					"üí° **Hint:** While a standard DataFrame has a simple `.count()` method, when performing operations such as `count` on a stream, you must use `.groupby()` before the aggregate operation."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"display(bronzeDF.groupby().count(), streamName=\"display_bronze_count\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Retrieve Second Month of Data\n",
					"\n",
					"Next, we use the utility function, `retrieve_data` to retrieve another file.\n",
					"\n",
					"After you ingest the file by running the following cell, view the streams above; you should be able to watch the data being ingested."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"retrieve_data(2020, 2, rawPath)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Write an Assertion Statement to Verify File Ingestion\n",
					"\n",
					"The expected file has the following name:"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"file_2020_2 = \"health_tracker_data_2020_2.json\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"assert file_2020_2 in [\n",
					"    item.name for item in dbutils.fs.ls(rawPath)\n",
					"], \"File not present in Raw Path\"\n",
					"print(\"Assertion passed.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Extracting Nested JSON\n",
					"\n",
					"We now begin the work of creating the Silver Table. First, we extract the JSON data from the `value` column in the Bronze Delta table. That this is being done after first landing our ingested data in a Bronze table means that we do not need to worry about the ingestion process breaking because the data did not parse.\n",
					"\n",
					"This extraction consists of two steps:\n",
					"\n",
					"1. We extract the nested JSON from `bronzeDF` using the `pyspark.sql` function `from_json`.\n",
					"\n",
					"   üìí The `from_json` function requires that a schema be passed as argument. Here we pass the schema `json_schema = \"device_id INTEGER, heartrate DOUBLE, name STRING, time FLOAT\"`.\n",
					"\n",
					"1. We flatten the nested JSON into a new DataFrame by selecting all nested values of the `nested_json` column."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import from_json\n",
					"\n",
					"json_schema = \"device_id INTEGER, heartrate DOUBLE, name STRING, time FLOAT\"\n",
					"\n",
					"silver_health_tracker = bronzeDF.select(\n",
					"    from_json(col(\"value\"), json_schema).alias(\"nested_json\")\n",
					").select(\"nested_json.*\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Transform the Data\n",
					"\n",
					"The \"time\" column isn't currently human-readable in Unix time format.\n",
					"We need to transform it to make it useful. We also extract just the date\n",
					"from the timestamp. Next, we transform `silver_health_tracker` with the\n",
					"following transformations:\n",
					"\n",
					"- convert the `time` column to a timestamp with the name `eventtime`\n",
					"- convert the `time` column to a date with the name `p_eventdate`\n",
					"\n",
					"Note that we name the new column `p_eventdate` to indicate that we are\n",
					"partitioning on this column."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col, from_unixtime\n",
					"\n",
					"silver_health_tracker = silver_health_tracker.select(\n",
					"    \"device_id\",\n",
					"    \"heartrate\",\n",
					"    from_unixtime(\"time\").cast(\"timestamp\").alias(\"eventtime\"),\n",
					"    \"name\",\n",
					"    from_unixtime(\"time\").cast(\"date\").alias(\"p_eventdate\"),\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Write an Assertion To Verify the Schema\n",
					"\n",
					"The DataFrame `silver_health_tracker` should now have the following schema:\n",
					"\n",
					"```\n",
					"device_id: integer\n",
					"heartrate: double\n",
					"eventtime: timestamp\n",
					"name: string\n",
					"p_eventdate: date```\n",
					"\n",
					"Write a schema using DDL format to complete the below assertion statement.\n",
					"\n",
					"üí™üèº Remember, the function `_parse_datatype_string` converts a DDL format schema string into a Spark schema."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"from pyspark.sql.types import _parse_datatype_string\n",
					"\n",
					"assert silver_health_tracker.schema == _parse_datatype_string(\n",
					"    \"\"\"\n",
					"  device_id INTEGER,\n",
					"  heartrate DOUBLE,\n",
					"  eventtime TIMESTAMP,\n",
					"  name STRING,\n",
					"  p_eventdate DATE\n",
					"\"\"\"\n",
					"), \"Schemas do not match\"\n",
					"print(\"Assertion passed.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## WRITE Stream to a Silver Table\n",
					"\n",
					"Next, we stream write to the Silver table.\n",
					"\n",
					"We partion this table on event data (`p_eventdate`)."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"(\n",
					"    silver_health_tracker.writeStream.format(\"delta\")\n",
					"    .outputMode(\"append\")\n",
					"    .option(\"checkpointLocation\", silverCheckpoint)\n",
					"    .partitionBy(\"p_eventdate\")\n",
					"    .queryName(\"write_bronze_to_silver\")\n",
					"    .start(silverPath)\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"\n",
					"untilStreamIsReady(\"write_bronze_to_silver\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark.sql(\n",
					"    \"\"\"\n",
					"DROP TABLE IF EXISTS health_tracker_plus_silver\n",
					"\"\"\"\n",
					")\n",
					"\n",
					"spark.sql(\n",
					"    f\"\"\"\n",
					"CREATE TABLE health_tracker_plus_silver\n",
					"USING DELTA\n",
					"LOCATION \"{silverPath}\"\n",
					"\"\"\"\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Troubleshooting\n",
					"\n",
					"üò´ If you try to run this before the `writeStream` above has been created, you may see the following error:\n",
					"\n",
					"`\n",
					"AnalysisException: Table schema is not set.  Write data into it or use CREATE TABLE to set the schema.;`\n",
					"\n",
					"If this happens, wait a moment for the `writeStream` to instantiate and run the command again."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Explore and Visualize the Data\n",
					"\n",
					"After running the following cell, click on \"Plot Options...\" and set the plot options as shown below:\n",
					"\n",
					"![Plot Options](https://files.training.databricks.com/images/pipelines_plot_options.png)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"display(\n",
					"    spark.readStream.table(\"health_tracker_plus_silver\"), streamName=\"display_silver\"\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### What patterns do you notice in the data? Anomalies?"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Missing Records\n",
					"\n",
					"When we look at the Silver table, we expect to see two months of data, five device measurements, 24 hours a day for (31 + 29) days, or 7200 records. (The data was recorded during the month of February in a leap year, which is why there are 29 days in the month.)\n",
					"\n",
					"‚ùóÔ∏èWe do not have a correct count. It looks like `device_id`: 4 is missing 72 records."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import count\n",
					"\n",
					"display(\n",
					"    spark.read.table(\"health_tracker_plus_silver\").groupby(\"device_id\").agg(count(\"*\"))\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Table Histories\n",
					"\n",
					"Recall that the Delta transaction log allows us to view all of the commits that have taken place in a Delta table's history."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from delta.tables import DeltaTable\n",
					"\n",
					"bronzeTable = DeltaTable.forPath(spark, bronzePath)\n",
					"silverTable = DeltaTable.forPath(spark, silverPath)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"display(bronzeTable.history())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"display(silverTable.history())"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Time Travel\n",
					"\n",
					"We can query an earlier version of the Delta table using the time travel feature. By running the following two cells, we can see that the current table count is larger than it was before we ingested the new data file into the stream."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT COUNT(*) FROM health_tracker_plus_bronze VERSION AS OF 0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT COUNT(*) FROM health_tracker_plus_bronze VERSION AS OF 1"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"%%sql\n",
					"\n",
					"SELECT COUNT(*) FROM health_tracker_plus_bronze"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Stop All Streams\n",
					"\n",
					"In the next notebook, we will analyze data in the Silver Delta table, and perform some update operations on the data.\n",
					"\n",
					"Before we do so, let's shut down all streams in this notebook."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"stop_all_streams()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"&copy; 2020 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}