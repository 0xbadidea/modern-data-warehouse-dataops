{
	"name": "04_silver_to_gold",
	"properties": {
		"folder": {
			"name": "Delta/pipelines"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "859fde54-3ddb-4411-8ef1-f144e29d068b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%md-sandbox\n",
					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Silver to Gold - Building Aggregate Data Marts for End Users\n",
					"\n",
					"We will now perform some aggregations on the data, as requested by one of our end users who wants to be able to quickly see summary statistics, aggregated by device id, in a dashboard in their chosen BI tool."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Notebook Objective\n",
					"\n",
					"In this notebook we:\n",
					"1. Create aggregations on the Silver table data\n",
					"1. Load the aggregate data into a Gold table"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Step Configuration"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ./includes/configuration"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Import Operation Functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ./includes/main/python/operations"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Display the Files in the Raw Paths"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"display(dbutils.fs.ls(rawPath))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Make Notebook Idempotent"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dbutils.fs.rm(goldPath, recurse=True)\n",
					"dbutils.fs.rm(goldCheckpoint, recurse=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Start Streams\n",
					"\n",
					"Before we add new streams, let's start the streams we have previously engineered.\n",
					"\n",
					"We will start two named streams:\n",
					"\n",
					"- `write_raw_to_bronze`\n",
					"- `write_bronze_to_silver`"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Current Delta Architecture\n",
					"\n",
					"Next, we demonstrate everything we have built up to this point in our\n",
					"Delta architecture.\n",
					"\n",
					"Again, we do so with composable functions included in the\n",
					"file `includes/main/python/operations`."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"rawDF = read_stream_raw(spark, rawPath)\n",
					"transformedRawDF = transform_raw(rawDF)\n",
					"rawToBronzeWriter = create_stream_writer(\n",
					"    dataframe=transformedRawDF,\n",
					"    checkpoint=bronzeCheckpoint,\n",
					"    name=\"write_raw_to_bronze\",\n",
					"    partition_column=\"p_ingestdate\",\n",
					")\n",
					"rawToBronzeWriter.start(bronzePath)\n",
					"\n",
					"bronzeDF = read_stream_delta(spark, bronzePath)\n",
					"transformedBronzeDF = transform_bronze(bronzeDF)\n",
					"bronzeToSilverWriter = create_stream_writer(\n",
					"    dataframe=transformedBronzeDF,\n",
					"    checkpoint=silverCheckpoint,\n",
					"    name=\"write_bronze_to_silver\",\n",
					"    partition_column=\"p_eventdate\",\n",
					")\n",
					"bronzeToSilverWriter.start(silverPath)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Update the Silver Table\n",
					"\n",
					"We periodically run the `update_silver_table` function to update the table and address the known issue of negative readings being ingested."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"update_silver_table(spark, silverPath)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Show Running Streams"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"for stream in spark.streams.active:\n",
					"    print(stream.name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Create Aggregation per User"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Create a read stream DataFrame and aggregate over the Silver table\n",
					"\n",
					"Use the following aggregates:\n",
					"- mean of heartrate, aliased as `mean_heartrate`\n",
					"- standard deviation of heartrate, aliased as `std_heartrate`\n",
					"- maximum of heartrate, aliased as `max_heartrate`"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"\n",
					"from pyspark.sql.functions import col, mean, stddev, max\n",
					"\n",
					"silverTableReadStream = read_stream_delta(spark, silverPath)\n",
					"\n",
					"gold_health_tracker_data_df = silverTableReadStream.groupBy(\"device_id\").agg(\n",
					"    mean(col(\"heartrate\")).alias(\"mean_heartrate\"),\n",
					"    stddev(col(\"heartrate\")).alias(\"std_heartrate\"),\n",
					"    max(col(\"heartrate\")).alias(\"max_heartrate\"),\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## WRITE Stream Gold Table Aggregation\n",
					"\n",
					"Note that we cannot use outputMode \"append\" for aggregations - we have to use \"complete\"."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"**Exercise:** Write the aggregate DataFrame to a Gold table"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"\n",
					"tableName = \"/aggregate_heartrate\"\n",
					"tableCheckpoint = goldCheckpoint + tableName\n",
					"tablePath = goldPath + tableName\n",
					"\n",
					"(\n",
					"    gold_health_tracker_data_df.writeStream.format(\"delta\")\n",
					"    .outputMode(\"complete\")\n",
					"    .option(\"checkpointLocation\", tableCheckpoint)\n",
					"    .queryName(\"write_silver_to_gold\")\n",
					"    .start(tablePath)\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# ANSWER\n",
					"\n",
					"untilStreamIsReady(\"write_silver_to_gold\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Register Gold Table in the Metastore"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark.sql(\n",
					"    \"\"\"\n",
					"DROP TABLE IF EXISTS health_tracker_gold_aggregate_heartrate\n",
					"\"\"\"\n",
					")\n",
					"\n",
					"spark.sql(\n",
					"    f\"\"\"\n",
					"CREATE TABLE health_tracker_gold_aggregate_heartrate\n",
					"USING DELTA\n",
					"LOCATION \"{tablePath}\"\n",
					"\"\"\"\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Troubleshooting\n",
					"\n",
					"ðŸ˜« If you try to run this before the `writeStream` above has been created, you may see the following error:\n",
					"\n",
					"`\n",
					"AnalysisException: Table schema is not set.  Write data into it or use CREATE TABLE to set the schema.;`\n",
					"\n",
					"If this happens, wait a moment for the `writeStream` to instantiate and run the command again."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"We could now use this `health_tracker_gold` Delta table to define a dashboard. The query used to create the table could be issued nightly to prepare the dashboard for the following business day, or as often as needed according to SLA requirements."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Stop All Streams\n",
					"\n",
					"In the next notebook, you will harden the Silver to Gold Step.\n",
					"\n",
					"Before we do so, let's shut down all streams in this notebook."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"stop_all_streams()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"&copy; 2020 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}