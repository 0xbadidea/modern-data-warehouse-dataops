{
	"name": "key-vault-backed-secret-scopes",
	"properties": {
		"folder": {
			"name": "Delta/resources"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "34cb02e4-cd7c-4eab-9286-6844f805fc65"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"\n",
					"%md-sandbox\n",
					"\n",
					"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
					"  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
					"</div>"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"#![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Key Vault-Backed Secret Scopes\n",
					"\n",
					"## Learning Objectives\n",
					"By the end of these lessons, you should be able to:\n",
					"* Configure Databricks to access Key Vault secrets\n",
					"* Read and write data directly from Blob Storage using secrets stored in Key Vault\n",
					"* Set different levels of access permission using SAS at the Storage service level\n",
					"* Mount Blob Storage into DBFS\n",
					"* Describe how mounting impacts secure access to data\n",
					"\n",
					"The overall goal of these three notebooks is to read and write data directly from Blob Storage using secrets stored in a Key Vault, accessed securely through the Databricks Secrets utility.\n",
					"\n",
					"This goal has been broken into 3 notebooks to make each step more digestible:\n",
					"1. 03a - Blob Storage - In the first notebook, we will add a file to a Blob on a Storage Account and generate SAS tokens with different permissions levels\n",
					"1. 03b - Key Vault - In the second notebook, we will configure an Azure Key Vault Access Policy and add text-based credentials as secrets\n",
					"1. 03c - Key Vault Backed Secret Scopes - In the third notebook, we will define a Secret Scope in Databircks by linking to the Key Vault and use the previously stored credentials to read and write from the Storage Container\n",
					"\n",
					"### Online Resources\n",
					"\n",
					"- [Azure Databricks Secrets](https://docs.azuredatabricks.net/user-guide/secrets/index.html)\n",
					"- [Azure Key Vault](https://docs.microsoft.com/en-us/azure/key-vault/key-vault-whatis)\n",
					"- [Azure Databricks DBFS](https://docs.azuredatabricks.net/user-guide/dbfs-databricks-file-system.html)\n",
					"- [Introduction to Azure Blob storage](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction)\n",
					"- [Databricks with Azure Blob Storage](https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html)\n",
					"- [Azure Data Lake Storage Gen1](https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-datalake.html#mount-azure-data-lake)\n",
					"- [Azure Data Lake Storage Gen2](https://docs.databricks.com/spark/latest/data-sources/azure/azure-datalake-gen2.html)"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) 03c - Key Vault Backed Secret Scopes\n",
					"\n",
					"In this notebook, we will use the Secret Scopes API to securely connect to the Key Vault. The Secret Scopes API will allow us to use the Blob Storage SAS tokens, stored as Secrets in the Key Vault, to read and write data from Blob Storage.\n",
					"\n",
					"### Learning Objectives\n",
					"By the end of this lesson, you should be able to:\n",
					"- Create a Secret Scope connected to Azure Key Vault\n",
					"- Mount Blob Storage to DBFS using a SAS token\n",
					"- Write data to Blob using a SAS token in Spark Configuration"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Classroom setup\n",
					"\n",
					"A quick script to define a username variable in Python and Scala."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%run ./Includes/User-Name"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"\n",
					"## Access Azure Databricks Secrets UI\n",
					"\n",
					"Now that you have an instance of Azure Key Vault up and running, it is time to let Azure Databricks know how to connect to it.\n",
					"\n",
					"The first step is to open a new web browser tab and navigate to `https://&lt;your_azure_databricks_url&gt;#secrets/createScope`\n",
					"\n",
					"<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The number after the `?o=` is the unique workspace identifier; append `#secrets/createScope` to this.\n",
					"\n",
					"<img src=\"https://files.training.databricks.com/images/adbcore/config-keyvault/db-secrets.png\" width=800px />"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Link Azure Databricks to Key Vault\n",
					"We'll be copy/pasting some values from the Azure Portal to this UI.\n",
					"\n",
					"In the Azure Portal on your Key Vault tab:\n",
					"1. Go to properties\n",
					"2. Copy and paste the DNS Name\n",
					"3. Copy and paste the Resource ID\n",
					"\n",
					"<img src=\"https://files.training.databricks.com/images/adbcore/config-keyvault/properties.png\" width=800px />"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Add configuration values to the Databricks Secret Scope UI that you copied from the Azure Key Vault\n",
					"\n",
					"\n",
					"In the Databricks Secrets UI:\n",
					"\n",
					"1. Enter the name of the secret scope; here, we'll use `students`.\n",
					"2. Paste the DNS Name\n",
					"3. Paste the Resource ID\n",
					"4. Click \"Create\"\n",
					"\n",
					"<img src=\"https://files.training.databricks.com/images/adbcore/config-keyvault/db-secrets-complete.png\" />\n",
					"\n",
					"  > MANAGE permission allows users to read and write to this secret scope, and, in the case of accounts on the Azure Databricks Premium Plan, to change permissions for the scope.\n",
					"\n",
					"  > Your account must have the Azure Databricks Premium Plan for you to be able to select Creator. This is the recommended approach: grant MANAGE permission to the Creator when you create the secret scope, and then assign more granular access permissions after you have tested the scope."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Apply Changes\n",
					"\n",
					"After a moment, you will see a dialog verifying that the secret scope has been created. Click \"Ok\" to close the box.\n",
					"\n",
					"<img src=\"https://files.training.databricks.com/images/adbcore/config-keyvault/db-secrets-confirm.png\" />"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### List Secret Scopes\n",
					"\n",
					"To list the existing secret scopes the `dbutils.secrets` utility can be used.\n",
					"\n",
					"You can list all scopes currently available in your workspace with:"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"dbutils.secrets.listScopes()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### List Secrets within a specific scope\n",
					"\n",
					"\n",
					"To list the secrets within a specific scope, you can supply that scope name."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"dbutils.secrets.list(\"students\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Using your Secrets\n",
					"\n",
					"To use your secrets, you supply the scope and key to the `get` method.\n",
					"\n",
					"Run the following cell to retrieve and print a secret."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\n",
					"print(dbutils.secrets.get(scope=\"students\", key=\"storageread\"))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Secrets are not displayed in clear text\n",
					"\n",
					"Notice that the value when printed out is `[REDACTED]`. This is to prevent your secrets from being exposed."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"## Mount Azure Blob Container - Read/List\n",
					"\n",
					"In this section, we'll demonstrating using a `SASTOKEN` that only has list and read permissions managed at the Storage Account level.\n",
					"\n",
					"**This means:**\n",
					"- Any user within the workspace can view and read the files mounted using this key\n",
					"- This key can be used to mount any container within the storage account with these privileges"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Unmount directory if previously mounted.\n",
					"MOUNTPOINT = \"/mnt/commonfiles\"\n",
					"if MOUNTPOINT in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:\n",
					"  dbutils.fs.unmount(MOUNTPOINT)\n",
					"\n",
					"# Add the Storage Account, Container, and reference the secret to pass the SAS Token\n",
					"STORAGE_ACCOUNT = dbutils.secrets.get(scope=\"students\", key=\"storageaccount\")\n",
					"CONTAINER = \"commonfiles\"\n",
					"SASTOKEN = dbutils.secrets.get(scope=\"students\", key=\"storageread\")\n",
					"\n",
					"# Do not change these values\n",
					"SOURCE = \"wasbs://{container}@{storage_acct}.blob.core.windows.net/\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\n",
					"URI = \"fs.azure.sas.{container}.{storage_acct}.blob.core.windows.net\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\n",
					"\n",
					"try:\n",
					"  dbutils.fs.mount(\n",
					"    source=SOURCE,\n",
					"    mount_point=MOUNTPOINT,\n",
					"    extra_configs={URI:SASTOKEN})\n",
					"except Exception as e:\n",
					"  if \"Directory already mounted\" in str(e):\n",
					"    pass # Ignore error if already mounted.\n",
					"  else:\n",
					"    raise e\n",
					"\n",
					"display(dbutils.fs.ls(MOUNTPOINT))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Define and display a Dataframe that reads a file from the mounted directory"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"salesDF = (spark.read\n",
					"              .option(\"header\", True)\n",
					"              .option(\"inferSchema\", True)\n",
					"              .csv(MOUNTPOINT + \"/sales.csv\"))\n",
					"\n",
					"display(salesDF)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Filter the Dataframe and display the results"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\n",
					"\n",
					"sales2004DF = salesDF.filter(\n",
					"    (col(\"ShipDateKey\") > 20031231) & (col(\"ShipDateKey\") <= 20041231)\n",
					")\n",
					"display(sales2004DF)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Details....\n",
					"\n",
					"\n",
					"While we can list and read files with this token, our job will abort when we try to write."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"try:\n",
					"  sales2004DF.write.mode(\"overwrite\").parquet(MOUNTPOINT + \"/sales2004\")\n",
					"except Exception as e:\n",
					"  print(e)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Review\n",
					"\n",
					"At this point you should see how to:\n",
					"* Use Secrets to access blobstorage\n",
					"* Mount the blobstore to dbfs (Data Bricks File System)\n",
					"\n",
					"Mounting data to dbfs makes that content available to anyone in that workspace.\n",
					"\n",
					"If you want to access blob store directly without mounting the rest of the notebook demonstrate that process."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Writing Directly to Blob using SAS token\n",
					"\n",
					"Note that when you mount a directory, by default, all users within the workspace will have the same privileges to interact with that directory. Here, we'll look at using a SAS token to directly write to a blob (without mounting). This ensures that only users with the workspace that have access to the associated key vault will be able to write."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark.conf.set(URI, dbutils.secrets.get(scope=\"students\", key=\"storagewrite\"))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Listing Directory Contents and writing using SAS token\n",
					"\n",
					"Because the configured container SAS gives us full permissions, we can interact with the blob storage using our `dbutils.fs` methods."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dbutils.fs.ls(SOURCE)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"We can write to this blob directly, without exposing this mount to others in our workspace."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"sales2004DF.write.mode(\"overwrite\").parquet(SOURCE + \"/sales2004\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dbutils.fs.ls(SOURCE)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Deleting using SAS token\n",
					"\n",
					"This scope also has delete permissions."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dbutils.fs.rm(SOURCE + \"/sales2004\", True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"### Cleaning up mounts\n",
					"\n",
					"If you don't explicitly unmount, the read-only blob that you mounted at the beginning of this notebook will remain accessible in your workspace."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"if MOUNTPOINT in [mnt.mountPoint for mnt in dbutils.fs.mounts()]:\n",
					"  dbutils.fs.unmount(MOUNTPOINT)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"## Congratulations!\n",
					"\n",
					"You should now be able to use the following tools in your workspace:\n",
					"\n",
					"* Databricks Secrets\n",
					"* Azure Key Vault\n",
					"* SAS token\n",
					"* dbutils.mount"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%md-sandbox\n",
					"&copy; 2020 Databricks, Inc. All rights reserved.<br/>\n",
					"Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
					"<br/>\n",
					"<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"
				],
				"execution_count": null
			}
		]
	}
}